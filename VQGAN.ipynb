{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ">[Blocks](#scrollTo=E5ULLjpO9mXo)\n",
        "\n",
        ">[Encoder](#scrollTo=7c6WG02L-Eq3)\n",
        "\n",
        ">[Decoder](#scrollTo=imM6vLFV-PG9)\n",
        "\n",
        ">[Codebook](#scrollTo=Jh1QSQlb-XBF)\n",
        "\n",
        ">[VQGAN](#scrollTo=rGVTl669_nYS)\n",
        "\n",
        ">[Discriminator](#scrollTo=96fvmTBd-uS3)\n",
        "\n",
        ">[LPIPS](#scrollTo=LlrRRNrTALgR)\n",
        "\n",
        ">[Utils](#scrollTo=5tRniRBaFEH2)\n",
        "\n",
        ">[Training VQGAN](#scrollTo=3YWWi__mFdCq)\n",
        "\n",
        ">[minigpt](#scrollTo=XFaeKlAYolpW)\n",
        "\n",
        ">[Tranformer](#scrollTo=1qDDvr8co4ZP)\n",
        "\n",
        ">[Training transformer](#scrollTo=7fnhX2B8pSGj)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "PkCBXg-robFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "metadata": {
        "id": "KEIHntevo-NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "\n",
        "  # Show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "pCAthGP_o-ND",
        "outputId": "5842a32c-62d0-408b-94dc-c71f471e2de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      const capture = document.createElement('button');\n",
              "      capture.textContent = 'Capture';\n",
              "      div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => capture.onclick = resolve);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to photo.jpg\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Blocks"
      ],
      "metadata": {
        "id": "E5ULLjpO9mXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxhNI8LYEpKH",
        "outputId": "319e8539-9af6-49fd-a275-a010d0d6b9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTQ1IiCz9lRx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GroupNorm(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(GroupNorm, self).__init__()\n",
        "        self.gn = nn.GroupNorm(num_groups=32, num_channels=channels, eps=1e-6, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gn(x)\n",
        "\n",
        "\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.block = nn.Sequential(\n",
        "            GroupNorm(in_channels),\n",
        "            Swish(),\n",
        "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "            GroupNorm(out_channels),\n",
        "            Swish(),\n",
        "            nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
        "        )\n",
        "\n",
        "        if in_channels != out_channels:\n",
        "            self.channel_up = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.in_channels != self.out_channels:\n",
        "            return self.channel_up(x) + self.block(x)\n",
        "        else:\n",
        "            return x + self.block(x)\n",
        "\n",
        "\n",
        "class UpSampleBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(UpSampleBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=2.0)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class DownSampleBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(DownSampleBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, 2, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pad = (0, 1, 0, 1)\n",
        "        x = F.pad(x, pad, mode=\"constant\", value=0)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class NonLocalBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(NonLocalBlock, self).__init__()\n",
        "        self.in_channels = channels\n",
        "\n",
        "        self.gn = GroupNorm(channels)\n",
        "        self.q = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "        self.k = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "        self.v = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "        self.proj_out = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = self.gn(x)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        b, c, h, w = q.shape\n",
        "\n",
        "        q = q.reshape(b, c, h*w)\n",
        "        q = q.permute(0, 2, 1)\n",
        "        k = k.reshape(b, c, h*w)\n",
        "        v = v.reshape(b, c, h*w)\n",
        "\n",
        "        attn = torch.bmm(q, k)\n",
        "        attn = attn * (int(c)**(-0.5))\n",
        "        attn = F.softmax(attn, dim=2)\n",
        "        attn = attn.permute(0, 2, 1)\n",
        "\n",
        "        A = torch.bmm(v, attn)\n",
        "        A = A.reshape(b, c, h, w)\n",
        "\n",
        "        return x + A\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder"
      ],
      "metadata": {
        "id": "7c6WG02L-Eq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "# from helper import ResidualBlock, NonLocalBlock, DownSampleBlock, UpSampleBlock, GroupNorm, Swish\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Encoder, self).__init__()\n",
        "        channels = [128, 128, 128, 256, 256, 512]\n",
        "        attn_resolutions = [16]\n",
        "        num_res_blocks = 2\n",
        "        resolution = 256\n",
        "        layers = [nn.Conv2d(args.image_channels, channels[0], 3, 1, 1)]\n",
        "        for i in range(len(channels)-1):\n",
        "            in_channels = channels[i]\n",
        "            out_channels = channels[i + 1]\n",
        "            for j in range(num_res_blocks):\n",
        "                layers.append(ResidualBlock(in_channels, out_channels))\n",
        "                in_channels = out_channels\n",
        "                if resolution in attn_resolutions:\n",
        "                    layers.append(NonLocalBlock(in_channels))\n",
        "            if i != len(channels)-2:\n",
        "                layers.append(DownSampleBlock(channels[i+1]))\n",
        "                resolution //= 2\n",
        "        layers.append(ResidualBlock(channels[-1], channels[-1]))\n",
        "        layers.append(NonLocalBlock(channels[-1]))\n",
        "        layers.append(ResidualBlock(channels[-1], channels[-1]))\n",
        "        layers.append(GroupNorm(channels[-1]))\n",
        "        layers.append(Swish())\n",
        "        layers.append(nn.Conv2d(channels[-1], args.latent_dim, 3, 1, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "WMHQPSgO93bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "imM6vLFV-PG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "# from helper import ResidualBlock, NonLocalBlock, UpSampleBlock, GroupNorm, Swish\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Decoder, self).__init__()\n",
        "        channels = [512, 256, 256, 128, 128]\n",
        "        attn_resolutions = [16]\n",
        "        num_res_blocks = 3\n",
        "        resolution = 16\n",
        "\n",
        "        in_channels = channels[0]\n",
        "        layers = [nn.Conv2d(args.latent_dim, in_channels, 3, 1, 1),\n",
        "                  ResidualBlock(in_channels, in_channels),\n",
        "                  NonLocalBlock(in_channels),\n",
        "                  ResidualBlock(in_channels, in_channels)]\n",
        "\n",
        "        for i in range(len(channels)):\n",
        "            out_channels = channels[i]\n",
        "            for j in range(num_res_blocks):\n",
        "                layers.append(ResidualBlock(in_channels, out_channels))\n",
        "                in_channels = out_channels\n",
        "                if resolution in attn_resolutions:\n",
        "                    layers.append(NonLocalBlock(in_channels))\n",
        "            if i != 0:\n",
        "                layers.append(UpSampleBlock(in_channels))\n",
        "                resolution *= 2\n",
        "\n",
        "        layers.append(GroupNorm(in_channels))\n",
        "        layers.append(Swish())\n",
        "        layers.append(nn.Conv2d(in_channels, args.image_channels, 3, 1, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "vgUJnecn-JOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codebook"
      ],
      "metadata": {
        "id": "Jh1QSQlb-XBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Codebook(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(Codebook, self).__init__()\n",
        "        self.num_codebook_vectors = args.num_codebook_vectors\n",
        "        self.latent_dim = args.latent_dim\n",
        "        self.beta = args.beta\n",
        "\n",
        "        self.embedding = nn.Embedding(self.num_codebook_vectors, self.latent_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.num_codebook_vectors, 1.0 / self.num_codebook_vectors)\n",
        "\n",
        "    def forward(self, z):\n",
        "        z = z.permute(0, 2, 3, 1).contiguous()\n",
        "        z_flattened = z.view(-1, self.latent_dim)\n",
        "\n",
        "        d = torch.sum(z_flattened**2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - \\\n",
        "            2*(torch.matmul(z_flattened, self.embedding.weight.t()))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "\n",
        "        loss = torch.mean((z_q.detach() - z)**2) + self.beta * torch.mean((z_q - z.detach())**2)\n",
        "\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        z_q = z_q.permute(0, 3, 1, 2)\n",
        "\n",
        "        return z_q, min_encoding_indices, loss"
      ],
      "metadata": {
        "id": "tudDYFR9-iid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VQGAN"
      ],
      "metadata": {
        "id": "rGVTl669_nYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# from encoder import Encoder\n",
        "# from decoder import Decoder\n",
        "# from codebook import Codebook\n",
        "\n",
        "\n",
        "class VQGAN(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(VQGAN, self).__init__()\n",
        "        self.encoder = Encoder(args).to(device=args.device)\n",
        "        self.decoder = Decoder(args).to(device=args.device)\n",
        "        self.codebook = Codebook(args).to(device=args.device)\n",
        "        self.quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim, 1).to(device=args.device)\n",
        "        self.post_quant_conv = nn.Conv2d(args.latent_dim, args.latent_dim, 1).to(device=args.device)\n",
        "\n",
        "    def forward(self, imgs):\n",
        "        encoded_images = self.encoder(imgs)\n",
        "        quant_conv_encoded_images = self.quant_conv(encoded_images)\n",
        "        codebook_mapping, codebook_indices, q_loss = self.codebook(quant_conv_encoded_images)\n",
        "        post_quant_conv_mapping = self.post_quant_conv(codebook_mapping)\n",
        "        decoded_images = self.decoder(post_quant_conv_mapping)\n",
        "\n",
        "        return decoded_images, codebook_indices, q_loss\n",
        "\n",
        "    def encode(self, imgs):\n",
        "        encoded_images = self.encoder(imgs)\n",
        "        quant_conv_encoded_images = self.quant_conv(encoded_images)\n",
        "        codebook_mapping, codebook_indices, q_loss = self.codebook(quant_conv_encoded_images)\n",
        "        return codebook_mapping, codebook_indices, q_loss\n",
        "\n",
        "    def decode(self, z):\n",
        "        post_quant_conv_mapping = self.post_quant_conv(z)\n",
        "        decoded_images = self.decoder(post_quant_conv_mapping)\n",
        "        return decoded_images\n",
        "\n",
        "    def calculate_lambda(self, perceptual_loss, gan_loss):\n",
        "        last_layer = self.decoder.model[-1]\n",
        "        last_layer_weight = last_layer.weight\n",
        "        perceptual_loss_grads = torch.autograd.grad(perceptual_loss, last_layer_weight, retain_graph=True)[0]\n",
        "        gan_loss_grads = torch.autograd.grad(gan_loss, last_layer_weight, retain_graph=True)[0]\n",
        "\n",
        "         = torch.norm(perceptual_loss_grads) / (torch.norm(gan_loss_grads) + 1e-4)\n",
        "         = torch.clamp(, 0, 1e4).detach()\n",
        "        return 0.8 * \n",
        "\n",
        "    @staticmethod\n",
        "    def adopt_weight(disc_factor, i, threshold, value=0.):\n",
        "        if i < threshold:\n",
        "            disc_factor = value\n",
        "        return disc_factor\n",
        "\n",
        "    def load_checkpoint(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b_nkb_M__miT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator"
      ],
      "metadata": {
        "id": "96fvmTBd-uS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PatchGAN Discriminator (https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py#L538)\n",
        "\"\"\"\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, args, num_filters_last=64, n_layers=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        layers = [nn.Conv2d(args.image_channels, num_filters_last, 4, 2, 1), nn.LeakyReLU(0.2)]\n",
        "        num_filters_mult = 1\n",
        "\n",
        "        for i in range(1, n_layers + 1):\n",
        "            num_filters_mult_last = num_filters_mult\n",
        "            num_filters_mult = min(2 ** i, 8)\n",
        "            layers += [\n",
        "                nn.Conv2d(num_filters_last * num_filters_mult_last, num_filters_last * num_filters_mult, 4,\n",
        "                          2 if i < n_layers else 1, 1, bias=False),\n",
        "                nn.BatchNorm2d(num_filters_last * num_filters_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        layers.append(nn.Conv2d(num_filters_last * num_filters_mult, 1, 4, 1, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "3G7FEKw_-qLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LPIPS"
      ],
      "metadata": {
        "id": "LlrRRNrTALgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import vgg16\n",
        "from collections import namedtuple\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "URL_MAP = {\n",
        "    \"vgg_lpips\": \"https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1\"\n",
        "}\n",
        "\n",
        "CKPT_MAP = {\n",
        "    \"vgg_lpips\": \"vgg.pth\"\n",
        "}\n",
        "\n",
        "\n",
        "def download(url, local_path, chunk_size=1024):\n",
        "    os.makedirs(os.path.split(local_path)[0], exist_ok=True)\n",
        "    with requests.get(url, stream=True) as r:\n",
        "        total_size = int(r.headers.get(\"content-length\", 0))\n",
        "        with tqdm(total=total_size, unit=\"B\", unit_scale=True) as pbar:\n",
        "            with open(local_path, \"wb\") as f:\n",
        "                for data in r.iter_content(chunk_size=chunk_size):\n",
        "                    if data:\n",
        "                        f.write(data)\n",
        "                        pbar.update(chunk_size)\n",
        "\n",
        "\n",
        "def get_ckpt_path(name, root):\n",
        "    assert name in URL_MAP\n",
        "    path = os.path.join(root, CKPT_MAP[name])\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Downloading {name} model from {URL_MAP[name]} to {path}\")\n",
        "        download(URL_MAP[name], path)\n",
        "    return path\n",
        "\n",
        "\n",
        "class LPIPS(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LPIPS, self).__init__()\n",
        "        self.scaling_layer = ScalingLayer()\n",
        "        self.channels = [64, 128, 256, 512, 512]\n",
        "        self.vgg = VGG16()\n",
        "        self.lins = nn.ModuleList([\n",
        "            NetLinLayer(self.channels[0]),\n",
        "            NetLinLayer(self.channels[1]),\n",
        "            NetLinLayer(self.channels[2]),\n",
        "            NetLinLayer(self.channels[3]),\n",
        "            NetLinLayer(self.channels[4])\n",
        "        ])\n",
        "\n",
        "        self.load_from_pretrained()\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def load_from_pretrained(self, name=\"vgg_lpips\"):\n",
        "        ckpt = get_ckpt_path(name, \"vgg_lpips\")\n",
        "        self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n",
        "\n",
        "    def forward(self, real_x, fake_x):\n",
        "        features_real = self.vgg(self.scaling_layer(real_x))\n",
        "        features_fake = self.vgg(self.scaling_layer(fake_x))\n",
        "        diffs = {}\n",
        "\n",
        "        for i in range(len(self.channels)):\n",
        "            diffs[i] = (norm_tensor(features_real[i]) - norm_tensor(features_fake[i])) ** 2\n",
        "\n",
        "        return sum([spatial_average(self.lins[i].model(diffs[i])) for i in range(len(self.channels))])\n",
        "\n",
        "\n",
        "class ScalingLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScalingLayer, self).__init__()\n",
        "        self.register_buffer(\"shift\", torch.Tensor([-.030, -.088, -.188])[None, :, None, None])\n",
        "        self.register_buffer(\"scale\", torch.Tensor([.458, .448, .450])[None, :, None, None])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return (x - self.shift) / self.scale\n",
        "\n",
        "\n",
        "class NetLinLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels=1):\n",
        "        super(NetLinLayer, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)\n",
        "        )\n",
        "\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        vgg_pretrained_features = vgg16(pretrained=True).features\n",
        "        slices = [vgg_pretrained_features[i] for i in range(30)]\n",
        "        self.slice1 = nn.Sequential(*slices[0:4])\n",
        "        self.slice2 = nn.Sequential(*slices[4:9])\n",
        "        self.slice3 = nn.Sequential(*slices[9:16])\n",
        "        self.slice4 = nn.Sequential(*slices[16:23])\n",
        "        self.slice5 = nn.Sequential(*slices[23:30])\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.slice1(x)\n",
        "        h_relu1 = h\n",
        "        h = self.slice2(h)\n",
        "        h_relu2 = h\n",
        "        h = self.slice3(h)\n",
        "        h_relu3 = h\n",
        "        h = self.slice4(h)\n",
        "        h_relu4 = h\n",
        "        h = self.slice5(h)\n",
        "        h_relu5 = h\n",
        "        vgg_outputs = namedtuple(\"VGGOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n",
        "        return vgg_outputs(h_relu1, h_relu2, h_relu3, h_relu4, h_relu5)\n",
        "\n",
        "\n",
        "def norm_tensor(x):\n",
        "    \"\"\"\n",
        "    Normalize images by their length to make them unit vector?\n",
        "    :param x: batch of images\n",
        "    :return: normalized batch of images\n",
        "    \"\"\"\n",
        "    norm_factor = torch.sqrt(torch.sum(x**2, dim=1, keepdim=True))\n",
        "    return x / (norm_factor + 1e-10)\n",
        "\n",
        "\n",
        "def spatial_average(x):\n",
        "    \"\"\"\n",
        "     imgs have: batch_size x channels x width x height --> average over width and height channel\n",
        "    :param x: batch of images\n",
        "    :return: averaged images along width and height\n",
        "    \"\"\"\n",
        "    return x.mean([2, 3], keepdim=True)"
      ],
      "metadata": {
        "id": "iIRBZ-L5_2LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "5tRniRBaFEH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import albumentations\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# --------------------------------------------- #\n",
        "#                  Data Utils\n",
        "# --------------------------------------------- #\n",
        "\n",
        "class ImagePaths(Dataset):\n",
        "    def __init__(self, path, size=None):\n",
        "        self.size = size\n",
        "\n",
        "        self.images = [os.path.join(path, file) for file in os.listdir(path)]\n",
        "        self._length = len(self.images)\n",
        "\n",
        "        self.rescaler = albumentations.SmallestMaxSize(max_size=self.size)\n",
        "        self.cropper = albumentations.CenterCrop(height=self.size, width=self.size)\n",
        "        self.preprocessor = albumentations.Compose([self.rescaler, self.cropper])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._length\n",
        "\n",
        "    def preprocess_image(self, image_path):\n",
        "        image = Image.open(image_path)\n",
        "        if not image.mode == \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "        image = np.array(image).astype(np.uint8)\n",
        "        image = self.preprocessor(image=image)[\"image\"]\n",
        "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
        "        image = image.transpose(2, 0, 1)\n",
        "        return image\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        example = self.preprocess_image(self.images[i])\n",
        "        return example\n",
        "\n",
        "\n",
        "def load_data(args):\n",
        "    train_data = ImagePaths(args.dataset_path, size=256)\n",
        "    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n",
        "    return train_loader\n",
        "\n",
        "\n",
        "# --------------------------------------------- #\n",
        "#                  Module Utils\n",
        "#            for Encoder, Decoder etc.\n",
        "# --------------------------------------------- #\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "def plot_images(images):\n",
        "    x = images[\"input\"]\n",
        "    reconstruction = images[\"rec\"]\n",
        "    half_sample = images[\"half_sample\"]\n",
        "    full_sample = images[\"full_sample\"]\n",
        "\n",
        "    fig, axarr = plt.subplots(1, 4)\n",
        "    axarr[0].imshow(x.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    axarr[1].imshow(reconstruction.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    axarr[2].imshow(half_sample.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    axarr[3].imshow(full_sample.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "HV3_On1kB4MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training VQGAN"
      ],
      "metadata": {
        "id": "3YWWi__mFdCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def configure_device(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    args.device = device\n",
        "    return args\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, losses, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'losses': losses,\n",
        "    }, path)\n",
        "\n",
        "def load_checkpoint(model, optimizer, path):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    return model, optimizer, checkpoint['epoch'], checkpoint['losses']"
      ],
      "metadata": {
        "id": "zxs8QNN5jKhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import utils as vutils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class TrainVQGAN:\n",
        "    def __init__(self, args):\n",
        "        self.vqgan = VQGAN(args).to(device=args.device)\n",
        "        self.discriminator = Discriminator(args).to(device=args.device)\n",
        "        self.discriminator.apply(weights_init)\n",
        "        self.perceptual_loss = LPIPS().eval().to(device=args.device)\n",
        "        self.opt_vq, self.opt_disc = self.configure_optimizers(args)\n",
        "\n",
        "        self.prepare_training()\n",
        "\n",
        "        self.train(args)\n",
        "\n",
        "    def configure_optimizers(self, args):\n",
        "        lr = args.learning_rate\n",
        "        opt_vq = torch.optim.Adam(\n",
        "            list(self.vqgan.encoder.parameters()) +\n",
        "            list(self.vqgan.decoder.parameters()) +\n",
        "            list(self.vqgan.codebook.parameters()) +\n",
        "            list(self.vqgan.quant_conv.parameters()) +\n",
        "            list(self.vqgan.post_quant_conv.parameters()),\n",
        "            lr=lr, eps=1e-08, betas=(args.beta1, args.beta2))\n",
        "        opt_disc = torch.optim.Adam(self.discriminator.parameters(),\n",
        "                                    lr=lr, eps=1e-08, betas=(args.beta1, args.beta2))\n",
        "\n",
        "        return opt_vq, opt_disc\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_training():\n",
        "        os.makedirs(\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/VQGAN_GeneratedImagesJ\", exist_ok=True)\n",
        "        os.makedirs(\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/checkpointsJ\", exist_ok=True)\n",
        "\n",
        "    def train(self, args):\n",
        "        train_dataset = load_data(args)\n",
        "        steps_per_epoch = len(train_dataset)\n",
        "\n",
        "        losses = []\n",
        "        discs = []\n",
        "        recons = []\n",
        "\n",
        "        for epoch in range(args.epochs):\n",
        "            epoch_losses = []\n",
        "            with tqdm(range(len(train_dataset))) as pbar:\n",
        "                for i, imgs in zip(pbar, train_dataset):\n",
        "                    imgs = imgs.to(device=args.device)\n",
        "\n",
        "                    decoded_images, _, q_loss = self.vqgan(imgs)\n",
        "\n",
        "                    disc_real = self.discriminator(imgs)\n",
        "                    disc_fake = self.discriminator(decoded_images)\n",
        "\n",
        "                    disc_factor = self.vqgan.adopt_weight(args.disc_factor, epoch * steps_per_epoch + i, threshold=args.disc_start)\n",
        "\n",
        "                    perceptual_loss = self.perceptual_loss(imgs, decoded_images)\n",
        "                    rec_loss = torch.abs(imgs - decoded_images)\n",
        "                    perceptual_rec_loss = args.perceptual_loss_factor * perceptual_loss + args.rec_loss_factor * rec_loss\n",
        "                    perceptual_rec_loss = perceptual_rec_loss.mean()\n",
        "                    g_loss = -torch.mean(disc_fake)\n",
        "\n",
        "                     = self.vqgan.calculate_lambda(perceptual_rec_loss, g_loss)\n",
        "                    vq_loss = perceptual_rec_loss + q_loss + disc_factor *  * g_loss\n",
        "\n",
        "                    d_loss_real = torch.mean(F.relu(1. - disc_real))\n",
        "                    d_loss_fake = torch.mean(F.relu(1. + disc_fake))\n",
        "                    gan_loss = disc_factor * 0.5 * (d_loss_real + d_loss_fake)\n",
        "\n",
        "                    self.opt_vq.zero_grad()\n",
        "                    vq_loss.backward(retain_graph=True)\n",
        "\n",
        "                    self.opt_disc.zero_grad()\n",
        "                    gan_loss.backward()\n",
        "\n",
        "                    self.opt_vq.step()\n",
        "                    self.opt_disc.step()\n",
        "\n",
        "                    epoch_losses.append(vq_loss.cpu().detach().numpy().item())\n",
        "\n",
        "            losses.append(np.mean(epoch_losses))\n",
        "\n",
        "            # Save the models every 2 epochs\n",
        "            if (epoch + 1) % 2 == 0:\n",
        "                torch.save(self.vqgan.state_dict(), os.path.join(\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/checkpointsJ\", f\"vqgan_epoch_{epoch}.pt\"))  # option 1 model\n",
        "\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': self.vqgan.state_dict(),\n",
        "                    'optimizer_state_dict': self.opt_vq.state_dict(),\n",
        "                    'losses': epoch_losses,\n",
        "                    },\n",
        "                    os.path.join(\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/checkpointsJ\", f\"vqgan_epoch_{epoch}.pt\"))\n",
        "\n",
        "                torch.save(self.discriminator.state_dict(), os.path.join(\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/checkpointsJ\", f\"discriminator_epoch_{epoch}.pt\"))\n",
        "\n",
        "            pbar.set_postfix(\n",
        "                VQ_Loss=np.round(vq_loss.cpu().detach().numpy().item(), 5),\n",
        "                GAN_Loss=np.round(gan_loss.cpu().detach().numpy().item(), 3)\n",
        "            )\n",
        "            pbar.update(0)\n",
        "\n",
        "\n",
        "            # if (epoch + 1) % 1 == 0:\n",
        "            #     with torch.no_grad():\n",
        "            #         for i in range(4):\n",
        "            #             # class_num = str(imgs[i].cpu().numpy()[0][0][0]).split('.')[0][-1]\n",
        "            #             class_num = imgs[i].cpu().numpy()[0][0][0].split('.')[0][-1]\n",
        "            #             class_num = int(class_num)\n",
        "            #             real_fake_images = torch.cat((imgs[i:i+1], decoded_images[i:i+1].add(1).mul(0.5)))\n",
        "            #             vutils.save_image(real_fake_images, os.path.join(\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/VQGAN_GeneratedImagesJ\", f\"real_fake_{epoch}_{class_num}.jpg\"))\n",
        "\n",
        "\n",
        "        # plt.plot(losses)\n",
        "        # plt.xlabel(\"Epoch\")\n",
        "        # plt.ylabel(\"VQ Loss\")\n",
        "        # plt.savefig(\"vq_loss.png\")\n",
        "\n",
        "        # plt.plot(discs)\n",
        "        # plt.xlabel(\"Epoch\")\n",
        "        # plt.ylabel(\"Discriminator Loss\")\n",
        "        # plt.savefig(\"disc_loss.png\")\n",
        "\n",
        "        # plt.plot(recons)\n",
        "        # plt.xlabel(\"Epoch\")\n",
        "        # plt.ylabel(\"Reconstruction Loss\")\n",
        "        # plt.savefig(\"rec_loss.png\")\n",
        "        plt.plot(losses)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"VQ Loss\")\n",
        "        plt.savefig(\"vq_loss.png\")\n",
        "\n",
        "        plt.plot(discs)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Discriminator Loss\")\n",
        "        plt.savefig(\"disc_loss.png\")\n",
        "\n",
        "        plt.plot(recons)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Reconstruction Loss\")\n",
        "        plt.savefig(\"rec_loss.png\")\n",
        "\n",
        "        plt.plot(lipips)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"LPIPS Loss\")\n",
        "        plt.savefig(\"lipip_loss.png\")\n",
        "\n",
        "\n",
        "    # def generate_images(self, num_images):\n",
        "    #     with torch.no_grad():\n",
        "    #         generated_images = self.vqgan.sample(num_images)\n",
        "    #         return generated_images\n",
        "\n",
        "    path=\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/metrics.txt\"\n",
        "    def save_metrics(self, path):\n",
        "        with open(path, \"w\") as f:  # \"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/metrics.txt\"\n",
        "            f.write(\"VQ Loss:\\n\")\n",
        "            for loss in self.losses:\n",
        "                f.write(f\"{loss}\\n\")\n",
        "\n",
        "            f.write(\"\\nDiscriminator Loss:\\n\")\n",
        "            for disc in self.discs:\n",
        "                f.write(f\"{disc}\\n\")\n",
        "\n",
        "            f.write(\"\\nReconstruction Loss:\\n\")\n",
        "            for recon in self.recons:\n",
        "                f.write(f\"{recon}\\n\")\n",
        "\n",
        "            f.write(\"\\nLPIPS Loss:\\n\")\n",
        "            for lipips in self.lipips:\n",
        "                f.write(f\"{lipips}\\n\")\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        plt.plot(self.losses)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"VQ Loss\")\n",
        "        plt.savefig(\"vq_loss.png\")\n",
        "\n",
        "        plt.plot(self.discs)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Discriminator Loss\")\n",
        "        plt.savefig(\"disc_loss.png\")\n",
        "\n",
        "        plt.plot(self.recons)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Reconstruction Loss\")\n",
        "        plt.savefig(\"rec_loss.png\")\n",
        "\n",
        "        plt.plot(self.lipips)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"LPIPS Loss\")\n",
        "        plt.savefig(\"lipip_loss.png\")\n",
        "\n",
        "\n",
        "    def load_metrics(self, path):\n",
        "        with open(path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "            losses = [float(line.strip()) for line in lines[2:]]\n",
        "            discs = [float(line.strip()) for line in lines[6:]]\n",
        "            recons = [float(line.strip()) for line in lines[10:]]\n",
        "            lipips = [float(line.strip()) for line in lines[14:]]\n",
        "\n",
        "        self.losses = losses\n",
        "        self.discs = discs\n",
        "        self.recons = recons\n",
        "        self.lipips = lipips\n",
        "\n"
      ],
      "metadata": {
        "id": "qx3o14_wXNk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Define default values for Colab environment\n",
        "    class Args:\n",
        "        latent_dim = 256\n",
        "        image_size = 256\n",
        "        num_codebook_vectors = 1024\n",
        "        beta = 0.25\n",
        "        image_channels = 3\n",
        "        dataset_path = '/data'\n",
        "        device = \"cuda\"\n",
        "        batch_size = 6\n",
        "        epochs = 100\n",
        "        learning_rate = 2.25e-05\n",
        "        beta1 = 0.5\n",
        "        beta2 = 0.9\n",
        "        disc_start = 10000\n",
        "        disc_factor = 1.0\n",
        "        rec_loss_factor = 1.0\n",
        "        perceptual_loss_factor = 1.0\n",
        "        # Define other default arguments here...\n",
        "\n",
        "    args = Args()\n",
        "    args.dataset_path = r\"/content/drive/MyDrive/FDU_HairFollicleDataset/Images\"\n",
        "else:\n",
        "    # Define argparse configuration as normal\n",
        "    parser = argparse.ArgumentParser(description=\"VQGAN\")\n",
        "    parser.add_argument('--latent-dim', type=int, default=256, help='Latent dimension n_z (default: 256)')\n",
        "    parser.add_argument('--image-size', type=int, default=256, help='Image height and width (default: 256)')\n",
        "    parser.add_argument('--num-codebook-vectors', type=int, default=1024, help='Number of codebook vectors (default: 256)')\n",
        "    parser.add_argument('--beta', type=float, default=0.25, help='Commitment loss scalar (default: 0.25)')\n",
        "    parser.add_argument('--image-channels', type=int, default=3, help='Number of channels of images (default: 3)')\n",
        "    parser.add_argument('--dataset-path', type=str, default='/data', help='Path to data (default: /data)')\n",
        "    parser.add_argument('--device', type=str, default=\"cuda\", help='Which device the training is on')\n",
        "    parser.add_argument('--batch-size', type=int, default=6, help='Input batch size for training (default: 6)')\n",
        "    parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train (default: 50)')\n",
        "    parser.add_argument('--learning-rate', type=float, default=2.25e-05, help='Learning rate (default: 0.0002)')\n",
        "    parser.add_argument('--beta1', type=float, default=0.5, help='Adam beta param (default: 0.0)')\n",
        "    parser.add_argument('--beta2', type=float, default=0.9, help='Adam beta param (default: 0.999)')\n",
        "    parser.add_argument('--disc-start', type=int, default=10000, help='When to start the discriminator (default: 0)')\n",
        "    parser.add_argument('--disc-factor', type=float, default=1., help='')\n",
        "    parser.add_argument('--rec-loss-factor', type=float, default=1., help='Weighting factor for reconstruction loss.')\n",
        "    parser.add_argument('--perceptual-loss-factor', type=float, default=1., help='Weighting factor for perceptual loss.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    args.dataset_path = r\"/content/drive/MyDrive/FDU_HairFollicleDataset/Images\"\n",
        "\n",
        "\n",
        "\n",
        "train_vqgan = TrainVQGAN(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "FE08IPCmwNKu",
        "outputId": "dcc4cd25-31e3-42a4-9e74-08fbafdb74c3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "  0%|          | 0/29 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 21.06 MiB is free. Process 2053 has 14.72 GiB memory in use. Of the allocated memory 14.49 GiB is allocated by PyTorch, and 89.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-a6c72f551d73>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mtrain_vqgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainVQGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-45a0d476e314>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-45a0d476e314>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                     \u001b[0mdecoded_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvqgan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0mdisc_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ca0318cce186>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, imgs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mencoded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mquant_conv_encoded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcodebook_mapping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodebook_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcodebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_conv_encoded_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-34597631a152>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-9abf7da4e29d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 21.06 MiB is free. Process 2053 has 14.72 GiB memory in use. Of the allocated memory 14.49 GiB is allocated by PyTorch, and 89.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import utils as vutils\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class TrainVQGAN:\n",
        "    def __init__(self, args):\n",
        "        self.losses = []\n",
        "        self.discs = []\n",
        "        self.recons = []\n",
        "        self.lipips = []\n",
        "        self.vqgan = VQGAN(args).to(device=args.device)\n",
        "        self.discriminator = Discriminator(args).to(device=args.device)\n",
        "        self.discriminator.apply(weights_init)\n",
        "        self.perceptual_loss = LPIPS().eval().to(device=args.device)\n",
        "        self.opt_vq, self.opt_disc = self.configure_optimizers(args)\n",
        "\n",
        "        self.prepare_training()\n",
        "\n",
        "        self.train(args)\n",
        "\n",
        "    def configure_optimizers(self, args):\n",
        "        lr = args.learning_rate\n",
        "        opt_vq = torch.optim.Adam(\n",
        "            list(self.vqgan.encoder.parameters()) +\n",
        "            list(self.vqgan.decoder.parameters()) +\n",
        "            list(self.vqgan.codebook.parameters()) +\n",
        "            list(self.vqgan.quant_conv.parameters()) +\n",
        "            list(self.vqgan.post_quant_conv.parameters()),\n",
        "            lr=lr, eps=1e-08, betas=(args.beta1, args.beta2))\n",
        "        opt_disc = torch.optim.Adam(self.discriminator.parameters(),\n",
        "                                    lr=lr, eps=1e-08, betas=(args.beta1, args.beta2))\n",
        "\n",
        "        return opt_vq, opt_disc\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_training():\n",
        "        os.makedirs(\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/VQGAN_GeneratedImagesJ\", exist_ok=True)\n",
        "        os.makedirs(\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/checkpointsJ\", exist_ok=True)\n",
        "\n",
        "    def train(self, args):\n",
        "        train_dataset = load_data(args)\n",
        "        steps_per_epoch = len(train_dataset)\n",
        "\n",
        "        # Load the checkpoint from the specified path\n",
        "        checkpoint_path = \"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/checkpoints/vqgan_epoch_14.pt\"\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "        # Load the model state dictionary from the checkpoint\n",
        "        self.vqgan.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "        # Load the optimizer state dictionary from the checkpoint\n",
        "        self.opt_vq.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Set the current epoch number to the epoch from which the model was loaded\n",
        "        current_epoch = checkpoint['epoch']\n",
        "\n",
        "        for epoch in range(current_epoch + 1, args.epochs + 1):\n",
        "          epoch_losses = []\n",
        "          epoch_discs = []\n",
        "          epoch_recons = []\n",
        "          epoch_lipips = []\n",
        "          with tqdm(range(len(train_dataset))) as pbar:\n",
        "            for i, imgs in zip(pbar, train_dataset):\n",
        "              imgs = imgs.to(device=args.device)\n",
        "\n",
        "              decoded_images, _, q_loss = self.vqgan(imgs)\n",
        "\n",
        "              disc_real = self.discriminator(imgs)\n",
        "              disc_fake = self.discriminator(decoded_images)\n",
        "\n",
        "              disc_factor = self.vqgan.adopt_weight(args.disc_factor, epoch * steps_per_epoch + i, threshold=args.disc_start)\n",
        "\n",
        "              perceptual_loss = self.perceptual_loss(imgs, decoded_images)\n",
        "              epoch_lipips.append(perceptual_loss.cpu().detach().numpy().item())\n",
        "\n",
        "              rec_loss = torch.abs(imgs - decoded_images)\n",
        "              perceptual_rec_loss = args.perceptual_loss_factor * perceptual_loss + args.rec_loss_factor * rec_loss\n",
        "              perceptual_rec_loss = perceptual_rec_loss.mean()\n",
        "              g_loss = -torch.mean(disc_fake)\n",
        "\n",
        "               = self.vqgan.calculate_lambda(perceptual_rec_loss, g_loss)\n",
        "              vq_loss = perceptual_rec_loss + q_loss + disc_factor *  * g_loss\n",
        "\n",
        "              d_loss_real = torch.mean(F.relu(1. - disc_real))\n",
        "              d_loss_fake = torch.mean(F.relu(1. + disc_fake))\n",
        "              gan_loss = disc_factor * 0.5 * (d_loss_real + d_loss_fake)\n",
        "\n",
        "              self.opt_vq.zero_grad()\n",
        "              vq_loss.backward(retain_graph=True)\n",
        "\n",
        "              self.opt_disc.zero_grad()\n",
        "              gan_loss.backward()\n",
        "\n",
        "              self.opt_vq.step()\n",
        "              self.opt_disc.step()\n",
        "\n",
        "              epoch_losses.append(vq_loss.cpu().detach().numpy().item())\n",
        "\n",
        "          self.losses.append(np.mean(epoch_losses))\n",
        "          self.discs.append(np.mean(epoch_discs))\n",
        "          self.recons.append(np.mean(epoch_recons))\n",
        "          self.lipips.append(np.mean(epoch_lipips))\n",
        "\n",
        "          # Save the models every 2 epochs\n",
        "          if (epoch + 1) % 2 == 0:\n",
        "            torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': self.vqgan.state_dict(),\n",
        "              'optimizer_state_dict': self.opt_vq.state_dict(),\n",
        "              'losses': epoch_losses,\n",
        "              },\n",
        "              os.path.join(\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/checkpointsJ\", f\"vqgan_epoch_{epoch}.pt\"))\n",
        "\n",
        "          pbar.set_postfix(\n",
        "            VQ_Loss=np.round(vq_loss.cpu().detach().numpy().item(), 5),\n",
        "            GAN_Loss=np.round(gan_loss.cpu().detach().numpy().item(), 3)\n",
        "          )\n",
        "          pbar.update(0)\n",
        "\n",
        "          # Generate images every 2 steps\n",
        "          # if (epoch + 1) % 2 == 0:\n",
        "          #   with torch.no_grad():\n",
        "          #     real_fake_images = torch.cat((imgs[:4], decoded_images.add(1).mul(0.5)))\n",
        "          #     vutils.save_image(real_fake_images, os.path.join(\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/VQGAN_GeneratedImagesJ\", f\"{epoch}.jpg\"))\n",
        "\n",
        "          if (epoch + 1) % 2 == 0:\n",
        "              with torch.no_grad():\n",
        "                  for i in range(4):\n",
        "                      class_num = imgs[i].numpy()[0][0][0]\n",
        "                      real_fake_images = torch.cat((imgs[i:i+1], decoded_images[i:i+1].add(1).mul(0.5)))\n",
        "                      vutils.save_image(real_fake_images, os.path.join(\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/VQGAN_GeneratedImagesJ\", f\"real_fake_{epoch}_{class_num}.jpg\"))\n",
        "        plt.plot(losses)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"VQ Loss\")\n",
        "        plt.savefig(\"vq_loss.png\")\n",
        "\n",
        "        plt.plot(discs)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Discriminator Loss\")\n",
        "        plt.savefig(\"disc_loss.png\")\n",
        "\n",
        "        plt.plot(recons)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Reconstruction Loss\")\n",
        "        plt.savefig(\"rec_loss.png\")\n",
        "\n",
        "        plt.plot(lipips)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"LPIPS Loss\")\n",
        "        plt.savefig(\"lipip_loss.png\")\n",
        "\n",
        "\n",
        "    def generate_images(self, num_images):\n",
        "        with torch.no_grad():\n",
        "            generated_images = self.vqgan.sample(num_images)\n",
        "            return generated_images\n",
        "\n",
        "\n",
        "    def save_metrics(self, path):\n",
        "        with open(path, \"w\") as f:\n",
        "            f.write(\"VQ Loss:\\n\")\n",
        "            for loss in self.losses:\n",
        "                f.write(f\"{loss}\\n\")\n",
        "\n",
        "            f.write(\"\\nDiscriminator Loss:\\n\")\n",
        "            for disc in self.discs:\n",
        "                f.write(f\"{disc}\\n\")\n",
        "\n",
        "            f.write(\"\\nReconstruction Loss:\\n\")\n",
        "            for recon in self.recons:\n",
        "                f.write(f\"{recon}\\n\")\n",
        "\n",
        "            f.write(\"\\nLPIPS Loss:\\n\")\n",
        "            for lipips in self.lipips:\n",
        "                f.write(f\"{lipips}\\n\")\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        plt.plot(self.losses)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"VQ Loss\")\n",
        "        plt.savefig(\"vq_loss.png\")\n",
        "\n",
        "        plt.plot(self.discs)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Discriminator Loss\")\n",
        "        plt.savefig(\"disc_loss.png\")\n",
        "\n",
        "        plt.plot(self.recons)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Reconstruction Loss\")\n",
        "        plt.savefig(\"rec_loss.png\")\n",
        "\n",
        "        plt.plot(self.lipips)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"LPIPS Loss\")\n",
        "        plt.savefig(\"lipip_loss.png\")\n",
        "\n",
        "\n",
        "    def load_metrics(self, path):\n",
        "        with open(path, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "\n",
        "            losses = [float(line.strip()) for line in lines[2:]]\n",
        "            discs = [float(line.strip()) for line in lines[6:]]\n",
        "            recons = [float(line.strip()) for line in lines[10:]]\n",
        "            lipips = [float(line.strip()) for line in lines[14:]]\n",
        "\n",
        "        self.losses = losses\n",
        "        self.discs = discs\n",
        "        self.recons = recons\n",
        "        self.lipips = lipips\n"
      ],
      "metadata": {
        "id": "o8FxCVhfXPUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Define default values for Colab environment\n",
        "    class Args:\n",
        "        latent_dim = 256\n",
        "        image_size = 256\n",
        "        num_codebook_vectors = 1024\n",
        "        beta = 0.25\n",
        "        image_channels = 3\n",
        "        dataset_path = '/data'\n",
        "        device = \"cuda\"\n",
        "        batch_size = 6\n",
        "        epochs = 100\n",
        "        learning_rate = 2.25e-05\n",
        "        beta1 = 0.5\n",
        "        beta2 = 0.9\n",
        "        disc_start = 10000\n",
        "        disc_factor = 1.0\n",
        "        rec_loss_factor = 1.0\n",
        "        perceptual_loss_factor = 1.0\n",
        "        # Define other default arguments here...\n",
        "\n",
        "    args = Args()\n",
        "    args.dataset_path = r\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/Images\"\n",
        "else:\n",
        "\n",
        "    # Define argparse configuration as normal\n",
        "    parser = argparse.ArgumentParser(description=\"VQGAN\")\n",
        "    parser.add_argument('--latent-dim', type=int, default=256, help='Latent dimension n_z (default: 256)')\n",
        "    parser.add_argument('--image-size', type=int, default=256, help='Image height and width (default: 256)')\n",
        "    parser.add_argument('--num-codebook-vectors', type=int, default=1024, help='Number of codebook vectors (default: 256)')\n",
        "    parser.add_argument('--beta', type=float, default=0.25, help='Commitment loss scalar (default: 0.25)')\n",
        "    parser.add_argument('--image-channels', type=int, default=3, help='Number of channels of images (default: 3)')\n",
        "    parser.add_argument('--dataset-path', type=str, default='/data', help='Path to data (default: /data)')\n",
        "    parser.add_argument('--device', type=str, default=\"cuda\", help='Which device the training is on')\n",
        "    parser.add_argument('--batch-size', type=int, default=6, help='Input batch size for training (default: 6)')\n",
        "    parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train (default: 50)')\n",
        "    parser.add_argument('--learning-rate', type=float, default=2.25e-05, help='Learning rate (default: 0.0002)')\n",
        "    parser.add_argument('--beta1', type=float, default=0.5, help='Adam beta param (default: 0.0)')\n",
        "    parser.add_argument('--beta2', type=float, default=0.9, help='Adam beta param (default: 0.999)')\n",
        "    parser.add_argument('--disc-start', type=int, default=10000, help='When to start the discriminator (default: 0)')\n",
        "    parser.add_argument('--disc-factor', type=float, default=1., help='')\n",
        "    parser.add_argument('--rec-loss-factor', type=float, default=1., help='Weighting factor for reconstruction loss.')\n",
        "    parser.add_argument('--perceptual-loss-factor', type=float, default=1., help='Weighting factor for perceptual loss.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    args.dataset_path = r\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/Images\"\n",
        "\n",
        "train_vqgan = TrainVQGAN(args)"
      ],
      "metadata": {
        "id": "SNPH7jI2nMHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Mkfj_0NmbtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import utils as vutils\n",
        "from discriminator import Discriminator\n",
        "from lpips import LPIPS\n",
        "from vqgan import VQGAN\n",
        "from utils import load_data, weights_init\n",
        "import time\n",
        "\n",
        "class TrainVQGAN:\n",
        "    def __init__(self, args):\n",
        "        self.vqgan = VQGAN(args).to(device=args.device)\n",
        "        self.discriminator = Discriminator(args).to(device=args.device)\n",
        "        self.discriminator.apply(weights_init)\n",
        "        self.perceptual_loss = LPIPS().eval().to(device=args.device)\n",
        "        self.opt_vq, self.opt_disc = self.configure_optimizers(args)\n",
        "\n",
        "        self.prepare_training()\n",
        "\n",
        "        if args.resume_training:\n",
        "            self.load_checkpoint(args)\n",
        "        else:\n",
        "            self.start_epoch = 0\n",
        "\n",
        "        self.train(args)\n",
        "\n",
        "\n",
        "    def configure_optimizers(self, args):\n",
        "        lr = args.learning_rate\n",
        "        opt_vq = torch.optim.Adam(\n",
        "            list(self.vqgan.encoder.parameters()) +\n",
        "            list(self.vqgan.decoder.parameters()) +\n",
        "            list(self.vqgan.codebook.parameters()) +\n",
        "            list(self.vqgan.quant_conv.parameters()) +\n",
        "            list(self.vqgan.post_quant_conv.parameters()),\n",
        "            lr=lr, eps=1e-08, betas=(args.beta1, args.beta2)\n",
        "        )\n",
        "        opt_disc = torch.optim.Adam(self.discriminator.parameters(),\n",
        "                                    lr=lr, eps=1e-08, betas=(args.beta1, args.beta2))\n",
        "\n",
        "        return opt_vq, opt_disc\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_training():\n",
        "        os.makedirs(\"kemal/results\", exist_ok=True)\n",
        "        os.makedirs(\"kemal/kemal checkpoints\", exist_ok=True)\n",
        "\n",
        "    def load_checkpoint(self, args):\n",
        "        checkpoint_file = \"kemal/kemal checkpoints/checkpoint_epoch_575.pt\"\n",
        "        checkpoint = torch.load(checkpoint_file)\n",
        "        self.start_epoch = checkpoint['epochs'][-1] + 1  # Start from the next epoch\n",
        "\n",
        "        self.vqgan.load_state_dict(checkpoint['vqgan_state_dict'])\n",
        "        self.discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "        self.opt_vq.load_state_dict(checkpoint['opt_vq_state_dict'])\n",
        "        self.opt_disc.load_state_dict(checkpoint['opt_disc_state_dict'])\n",
        "\n",
        "\n",
        "    def train(self, args):\n",
        "        train_dataset = load_data(args)\n",
        "        steps_per_epoch = len(train_dataset)\n",
        "        checkpoint_data = {\n",
        "            'epochs': [],\n",
        "            'vq_losses': [],\n",
        "            'gan_losses': [],\n",
        "            'vqgan_state_dict': self.vqgan.state_dict(),\n",
        "            'discriminator_state_dict': self.discriminator.state_dict(),\n",
        "            'opt_vq_state_dict': self.opt_vq.state_dict(),\n",
        "            'opt_disc_state_dict': self.opt_disc.state_dict()\n",
        "        }\n",
        "        for epoch in range(self.start_epoch, args.epochs):\n",
        "            start_time = time.time()\n",
        "            epoch_losses = {'vq_losses': [], 'gan_losses': []}\n",
        "            for i, imgs in enumerate(train_dataset):\n",
        "                    imgs = imgs.to(device=args.device)\n",
        "                    decoded_images, _, q_loss = self.vqgan(imgs)\n",
        "\n",
        "                    disc_real = self.discriminator(imgs)\n",
        "                    disc_fake = self.discriminator(decoded_images)\n",
        "\n",
        "                    disc_factor = self.vqgan.adopt_weight(args.disc_factor, epoch*steps_per_epoch+i, threshold=args.disc_start)\n",
        "\n",
        "                    perceptual_loss = self.perceptual_loss(imgs, decoded_images)\n",
        "                    rec_loss = torch.abs(imgs - decoded_images)\n",
        "                    perceptual_rec_loss = args.perceptual_loss_factor * perceptual_loss + args.rec_loss_factor * rec_loss\n",
        "                    perceptual_rec_loss = perceptual_rec_loss.mean()\n",
        "                    g_loss = -torch.mean(disc_fake)\n",
        "\n",
        "                     = self.vqgan.calculate_lambda(perceptual_rec_loss, g_loss)\n",
        "                    vq_loss = perceptual_rec_loss + q_loss + disc_factor *  * g_loss\n",
        "\n",
        "                    d_loss_real = torch.mean(F.relu(1. - disc_real))\n",
        "                    d_loss_fake = torch.mean(F.relu(1. + disc_fake))\n",
        "                    gan_loss = disc_factor * 0.5*(d_loss_real + d_loss_fake)\n",
        "\n",
        "                    self.opt_vq.zero_grad()\n",
        "                    vq_loss.backward(retain_graph=True)\n",
        "\n",
        "                    self.opt_disc.zero_grad()\n",
        "                    gan_loss.backward()\n",
        "\n",
        "                    self.opt_vq.step()\n",
        "                    self.opt_disc.step()\n",
        "                    epoch_losses['vq_losses'].append(vq_loss.cpu().detach().numpy().item())\n",
        "                    epoch_losses['gan_losses'].append(gan_loss.cpu().detach().numpy().item())\n",
        "                    epoch_losses['vqgan_state_dict'] = self.vqgan.state_dict()\n",
        "                    epoch_losses['discriminator_state_dict'] = self.discriminator.state_dict()\n",
        "                    epoch_losses['opt_vq_state_dict'] = self.opt_vq.state_dict()\n",
        "                    epoch_losses['opt_disc_state_dict'] = self.opt_disc.state_dict()\n",
        "            checkpoint_data['epochs'].append(epoch)\n",
        "            checkpoint_data['vq_losses'].append(vq_loss.cpu().detach().numpy().item())\n",
        "            checkpoint_data['gan_losses'].append(gan_loss.cpu().detach().numpy().item())\n",
        "            checkpoint_data['vqgan_state_dict'] = self.vqgan.state_dict()\n",
        "            checkpoint_data['discriminator_state_dict'] = self.discriminator.state_dict()\n",
        "            checkpoint_data['opt_vq_state_dict'] = self.opt_vq.state_dict()\n",
        "            checkpoint_data['opt_disc_state_dict'] = self.opt_disc.state_dict()\n",
        "            if epoch % 5 == 0:  # Save every five epochs\n",
        "                torch.save(self.vqgan.state_dict(), os.path.join(\"kemal/kemal checkpoints\", f\"vqgan_epoch_{epoch}.pt\"))\n",
        "                torch.save(self.discriminator.state_dict(), os.path.join(\"kemal/kemal checkpoints\", f\"discriminator_epoch_{epoch}.pt\"))\n",
        "                torch.save(self.opt_vq.state_dict(), os.path.join(\"kemal/kemal checkpoints\", f\"opt_vq_epoch_{epoch}.pt\"))\n",
        "                torch.save(self.opt_disc.state_dict(), os.path.join(\"kemal/kemal checkpoints\", f\"opt_disc_epoch_{epoch}.pt\"))\n",
        "                torch.save(checkpoint_data, os.path.join(\"kemal/kemal checkpoints\", f\"checkpoint_epoch_{epoch}.pt\"))\n",
        "            print(f\"Epoch: {epoch}, Step: {i}, VQ_Loss: {vq_loss.cpu().detach().numpy().item()}, GAN_Loss: {gan_loss.cpu().detach().numpy().item()}\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for j, img in enumerate(decoded_images):\n",
        "                    vutils.save_image(img, os.path.join(\"kemal/results\", f\"epoch_{epoch}_img_{j}.jpg\"))\n",
        "            pbar = tqdm(total=steps_per_epoch, desc=f\"Epoch: {epoch}, Step: {i}\", leave=False)\n",
        "            pbar.set_postfix(\n",
        "                VQ_Loss=np.round(vq_loss.cpu().detach().numpy().item(), 5),\n",
        "                GAN_Loss=np.round(gan_loss.cpu().detach().numpy().item(), 3)\n",
        "            )\n",
        "            pbar.update(0)\n",
        "            pbar.update(1)\n",
        "            # Calculate and print the time taken for the epoch\n",
        "            epoch_time = time.time() - start_time\n",
        "            print(f\"Time taken for Epoch {epoch}: {epoch_time / 60} minutes\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description=\"VQGAN\")\n",
        "    parser.add_argument('--latent-dim', type=int, default=256, help='Latent dimension n_z (default: 256)')\n",
        "    parser.add_argument('--image-size', type=int, default=256, help='Image height and width (default: 256)')\n",
        "    parser.add_argument('--num-codebook-vectors', type=int, default=1024, help='Number of codebook vectors (default: 256)')\n",
        "    parser.add_argument('--beta', type=float, default=0.25, help='Commitment loss scalar (default: 0.25)')\n",
        "    parser.add_argument('--image-channels', type=int, default=3, help='Number of channels of images (default: 3)')\n",
        "    parser.add_argument('--dataset-path', type=str, default='/data', help='Path to data (default: /data)')\n",
        "    parser.add_argument('--device', type=str, default=\"cuda\", help='Which device the training is on')\n",
        "    parser.add_argument('--batch-size', type=int, default=6, help='Input batch size for training (default: 6)')\n",
        "    parser.add_argument('--epochs', type=int, default=2000, help='Number of epochs to train (default: 50)')\n",
        "    parser.add_argument('--learning-rate', type=float, default=2.25e-05, help='Learning rate (default: 0.0002)')\n",
        "    parser.add_argument('--beta1', type=float, default=0.5, help='Adam beta param (default: 0.0)')\n",
        "    parser.add_argument('--beta2', type=float, default=0.9, help='Adam beta param (default: 0.999)')\n",
        "    parser.add_argument('--disc-start', type=int, default=10000, help='When to start the discriminator (default: 0)')\n",
        "    parser.add_argument('--disc-factor', type=float, default=1., help='')\n",
        "    parser.add_argument('--rec-loss-factor', type=float, default=1., help='Weighting factor for reconstruction loss.')\n",
        "    parser.add_argument('--perceptual-loss-factor', type=float, default=1., help='Weighting factor for perceptual loss.')\n",
        "    parser.add_argument('--save-interval', type=int, default=1, help='Interval for saving model checkpoints (default: 1)')\n",
        "    parser.add_argument('--resume-training', action='store_true', help='Resume training from a checkpoint')\n",
        "    parser.add_argument('--checkpoint-file', type=str, default='', help='Path to the checkpoint file to resume training')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    args.dataset_path = r\"kemal/Images\"\n",
        "    args.resume_training = True  # Set to False for initial training\n",
        "    # checkpoint_file = 'kemal/kemal checkpoints/checkpoint_epoch_120.pt'  # Provide the path to your checkpoint file\n",
        "    checkpoint_file = '/home/ge22m009/kemal/kemal checkpoints/checkpoint_epoch_575.pt'\n",
        "\n",
        "    train_vqgan = TrainVQGAN(args)"
      ],
      "metadata": {
        "id": "90Rv9Hfra_-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aRgoCd5BwJQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# minigpt"
      ],
      "metadata": {
        "id": "XFaeKlAYolpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "taken from: https://github.com/karpathy/minGPT/\n",
        "GPT model:\n",
        "- the initial stem consists of a combination of token encoding and a positional encoding\n",
        "- the meat of it is a uniform sequence of Transformer blocks\n",
        "    - each Transformer is a sequential combination of a 1-hidden-layer MLP block and a self-attention block\n",
        "    - all blocks feed into a central residual pathway similar to resnets\n",
        "- the final decoder is a linear projection into a vanilla Softmax classifier\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class GPTConfig:\n",
        "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
        "    embd_pdrop = 0.1\n",
        "    resid_pdrop = 0.1\n",
        "    attn_pdrop = 0.1\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
        "    explicit implementation here to show that there is nothing too scary here.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads\n",
        "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # regularization\n",
        "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        mask = torch.tril(torch.ones(config.block_size,\n",
        "                                     config.block_size))\n",
        "        if hasattr(config, \"n_unmasked\"):\n",
        "            mask[:config.n_unmasked, :config.n_unmasked] = 1\n",
        "        self.register_buffer(\"mask\", mask.view(1, 1, config.block_size, config.block_size))\n",
        "        self.n_head = config.n_head\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        B, T, C = x.size()\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
        "\n",
        "        present = torch.stack((k, v))\n",
        "        if layer_past is not None:\n",
        "            past_key, past_value = layer_past\n",
        "            k = torch.cat((past_key, k), dim=-2)\n",
        "            v = torch.cat((past_value, v), dim=-2)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        if layer_past is None:\n",
        "            att = att.masked_fill(self.mask[:, :, :T, :T] == 0, float('-inf'))\n",
        "\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_drop(att)\n",
        "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_drop(self.proj(y))\n",
        "        return y, present  # TODO: check that this does not break anything\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" an unassuming Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
        "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            nn.GELU(),  # nice\n",
        "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            nn.Dropout(config.resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, layer_past=None, return_present=False):\n",
        "        # TODO: check that training still works\n",
        "        if return_present:\n",
        "            assert not self.training\n",
        "        # layer past: tuple of length two with B, nh, T, hs\n",
        "        attn, present = self.attn(self.ln1(x), layer_past=layer_past)\n",
        "\n",
        "        x = x + attn\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        if layer_past is not None or return_present:\n",
        "            return x, present\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, n_layer=12, n_head=8, n_embd=256,\n",
        "                 embd_pdrop=0., resid_pdrop=0., attn_pdrop=0., n_unmasked=0):\n",
        "        super().__init__()\n",
        "        config = GPTConfig(vocab_size=vocab_size, block_size=block_size,\n",
        "                           embd_pdrop=embd_pdrop, resid_pdrop=resid_pdrop, attn_pdrop=attn_pdrop,\n",
        "                           n_layer=n_layer, n_head=n_head, n_embd=n_embd,\n",
        "                           n_unmasked=n_unmasked)\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))  # 512 x 1024\n",
        "        self.drop = nn.Dropout(config.embd_pdrop)\n",
        "        # transformer\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
        "        self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        self.block_size = config.block_size\n",
        "        self.apply(self._init_weights)\n",
        "        self.config = config\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def forward(self, idx, embeddings=None):\n",
        "        token_embeddings = self.tok_emb(idx)  # each index maps to a (learnable) vector\n",
        "\n",
        "        if embeddings is not None:  # prepend explicit embeddings\n",
        "            token_embeddings = torch.cat((embeddings, token_embeddings), dim=1)\n",
        "\n",
        "        t = token_embeddings.shape[1]\n",
        "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "        position_embeddings = self.pos_emb[:, :t, :]  # each position maps to a (learnable) vector\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        return logits, None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cmQZR52yooIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VG8PEpFYoqBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tranformer"
      ],
      "metadata": {
        "id": "1qDDvr8co4ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from mingpt import GPT\n",
        "# from vqgan import VQGAN\n",
        "\n",
        "\n",
        "class VQGANTransformer(nn.Module):\n",
        "    def __init__(self, args):\n",
        "        super(VQGANTransformer, self).__init__()\n",
        "\n",
        "        self.sos_token = args.sos_token\n",
        "\n",
        "        self.vqgan = self.load_vqgan(args)\n",
        "\n",
        "        transformer_config = {\n",
        "            \"vocab_size\": args.num_codebook_vectors,\n",
        "            \"block_size\": 512,\n",
        "            \"n_layer\": 24,\n",
        "            \"n_head\": 16,\n",
        "            \"n_embd\": 1024\n",
        "        }\n",
        "        self.transformer = GPT(**transformer_config)\n",
        "\n",
        "        self.pkeep = args.pkeep\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vqgan(args):\n",
        "        model = VQGAN(args)\n",
        "        model.load_checkpoint(args.checkpoint_path)\n",
        "        model = model.eval()\n",
        "        return model\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_to_z(self, x):\n",
        "        quant_z, indices, _ = self.vqgan.encode(x)\n",
        "        indices = indices.view(quant_z.shape[0], -1)\n",
        "        return quant_z, indices\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def z_to_image(self, indices, p1=16, p2=16):\n",
        "        ix_to_vectors = self.vqgan.codebook.embedding(indices).reshape(indices.shape[0], p1, p2, 256)\n",
        "        ix_to_vectors = ix_to_vectors.permute(0, 3, 1, 2)\n",
        "        image = self.vqgan.decode(ix_to_vectors)\n",
        "        return image\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, indices = self.encode_to_z(x)\n",
        "\n",
        "        sos_tokens = torch.ones(x.shape[0], 1) * self.sos_token\n",
        "        sos_tokens = sos_tokens.long().to(\"cuda\")\n",
        "\n",
        "        mask = torch.bernoulli(self.pkeep * torch.ones(indices.shape, device=indices.device))\n",
        "        mask = mask.round().to(dtype=torch.int64)\n",
        "        random_indices = torch.randint_like(indices, self.transformer.config.vocab_size)\n",
        "        new_indices = mask * indices + (1 - mask) * random_indices\n",
        "\n",
        "        new_indices = torch.cat((sos_tokens, new_indices), dim=1)\n",
        "\n",
        "        target = indices\n",
        "\n",
        "        logits, _ = self.transformer(new_indices[:, :-1])\n",
        "\n",
        "        return logits, target\n",
        "\n",
        "    def top_k_logits(self, logits, k):\n",
        "        v, ix = torch.topk(logits, k)\n",
        "        out = logits.clone()\n",
        "        out[out < v[..., [-1]]] = -float(\"inf\")\n",
        "        return out\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, x, c, steps, temperature=1.0, top_k=100):\n",
        "        self.transformer.eval()\n",
        "        x = torch.cat((c, x), dim=1)\n",
        "        for k in range(steps):\n",
        "            logits, _ = self.transformer(x)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                logits = self.top_k_logits(logits, top_k)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            x = torch.cat((x, ix), dim=1)\n",
        "\n",
        "        x = x[:, c.shape[1]:]\n",
        "        self.transformer.train()\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def log_images(self, x):\n",
        "        log = dict()\n",
        "\n",
        "        _, indices = self.encode_to_z(x)\n",
        "        sos_tokens = torch.ones(x.shape[0], 1) * self.sos_token\n",
        "        sos_tokens = sos_tokens.long().to(\"cuda\")\n",
        "\n",
        "        start_indices = indices[:, :indices.shape[1] // 2]\n",
        "        sample_indices = self.sample(start_indices, sos_tokens, steps=indices.shape[1] - start_indices.shape[1])\n",
        "        half_sample = self.z_to_image(sample_indices)\n",
        "\n",
        "        start_indices = indices[:, :0]\n",
        "        sample_indices = self.sample(start_indices, sos_tokens, steps=indices.shape[1])\n",
        "        full_sample = self.z_to_image(sample_indices)\n",
        "\n",
        "        x_rec = self.z_to_image(indices)\n",
        "\n",
        "        log[\"input\"] = x\n",
        "        log[\"rec\"] = x_rec\n",
        "        log[\"half_sample\"] = half_sample\n",
        "        log[\"full_sample\"] = full_sample\n",
        "\n",
        "        return log, torch.concat((x, x_rec, half_sample, full_sample))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KgeDjrYMo7ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oIeZufC8o__F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training transformer"
      ],
      "metadata": {
        "id": "7fnhX2B8pSGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import utils as vutils\n",
        "# from transformer import VQGANTransformer\n",
        "# from utils import load_data, plot_images\n",
        "\n",
        "\n",
        "class TrainTransformer:\n",
        "    def __init__(self, args):\n",
        "        self.model = VQGANTransformer(args).to(device=args.device)\n",
        "        self.optim = self.configure_optimizers()\n",
        "\n",
        "        self.train(args)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        decay, no_decay = set(), set()\n",
        "        whitelist_weight_modules = (nn.Linear, )\n",
        "        blacklist_weight_modules = (nn.LayerNorm, nn.Embedding)\n",
        "\n",
        "        for mn, m in self.model.transformer.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = f\"{mn}.{pn}\" if mn else pn\n",
        "\n",
        "                if pn.endswith(\"bias\"):\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "                elif pn.endswith(\"weight\") and isinstance(m, whitelist_weight_modules):\n",
        "                    decay.add(fpn)\n",
        "\n",
        "                elif pn.endswith(\"weight\") and isinstance(m, blacklist_weight_modules):\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        no_decay.add(\"pos_emb\")\n",
        "\n",
        "        param_dict = {pn: p for pn, p in self.model.transformer.named_parameters()}\n",
        "\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": 0.01},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=4.5e-06, betas=(0.9, 0.95))\n",
        "        return optimizer\n",
        "\n",
        "    def train(self, args):\n",
        "        train_dataset = load_data(args)\n",
        "        for epoch in range(args.epochs):\n",
        "            with tqdm(range(len(train_dataset))) as pbar:\n",
        "                for i, imgs in zip(pbar, train_dataset):\n",
        "                    self.optim.zero_grad()\n",
        "                    imgs = imgs.to(device=args.device)\n",
        "                    logits, targets = self.model(imgs)\n",
        "                    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
        "                    loss.backward()\n",
        "                    self.optim.step()\n",
        "                    pbar.set_postfix(Transformer_Loss=np.round(loss.cpu().detach().numpy().item(), 4))\n",
        "                    pbar.update(0)\n",
        "            log, sampled_imgs = self.model.log_images(imgs[0][None])\n",
        "            vutils.save_image(sampled_imgs, os.path.join(\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/transformer_gen\", f\"transformer_{epoch}.jpg\"), nrow=4)\n",
        "            plot_images(log)\n",
        "            torch.save(self.model.state_dict(), os.path.join(\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/checkpt\", f\"transformer_{epoch}.pt\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "UViGjqbTpVQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import sys\n",
        "\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Define default values for Colab environment\n",
        "    class Args:\n",
        "      latent_dim = 256\n",
        "      image_size = 256\n",
        "      num_codebook_vectors = 1024\n",
        "      beta = 0.25\n",
        "      image_channels = 3\n",
        "      dataset_path = '/data'\n",
        "      device = \"cuda\"\n",
        "      batch_size = 6\n",
        "      epochs = 1000\n",
        "      learning_rate = 2.25e-05\n",
        "      beta1 = 0.5\n",
        "      beta2 = 0.9\n",
        "      disc_start = 10000\n",
        "      disc_factor = 1.0\n",
        "      rec_loss_factor = 1.0\n",
        "      perceptual_loss_factor = 1.0\n",
        "\n",
        "      pkeep=0.5\n",
        "      sos_token=0\n",
        "      # Define other default arguments here...\n",
        "\n",
        "\n",
        "    args = Args()\n",
        "    args.dataset_path = r\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/Images\"\n",
        "    args.checkpoint_path =\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/checkpoints/vqgan_epoch_999.pt\"\n",
        "else:\n",
        "    # Define argparse configuration as normal\n",
        "    parser = argparse.ArgumentParser(description=\"VQGAN\")\n",
        "    parser.add_argument('--latent-dim', type=int, default=256, help='Latent dimension n_z (default: 256)')\n",
        "    parser.add_argument('--image-size', type=int, default=256, help='Image height and width (default: 256)')\n",
        "    parser.add_argument('--num-codebook-vectors', type=int, default=1024, help='Number of codebook vectors (default: 256)')\n",
        "    parser.add_argument('--beta', type=float, default=0.25, help='Commitment loss scalar (default: 0.25)')\n",
        "    parser.add_argument('--image-channels', type=int, default=3, help='Number of channels of images (default: 3)')\n",
        "    parser.add_argument('--dataset-path', type=str, default='/data', help='Path to data (default: /data)')\n",
        "    parser.add_argument('--device', type=str, default=\"cuda\", help='Which device the training is on')\n",
        "    parser.add_argument('--batch-size', type=int, default=6, help='Input batch size for training (default: 6)')\n",
        "    parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train (default: 50)')\n",
        "    parser.add_argument('--learning-rate', type=float, default=2.25e-05, help='Learning rate (default: 0.0002)')\n",
        "    parser.add_argument('--beta1', type=float, default=0.5, help='Adam beta param (default: 0.0)')\n",
        "    parser.add_argument('--beta2', type=float, default=0.9, help='Adam beta param (default: 0.999)')\n",
        "    parser.add_argument('--disc-start', type=int, default=10000, help='When to start the discriminator (default: 0)')\n",
        "    parser.add_argument('--disc-factor', type=float, default=1., help='')\n",
        "    parser.add_argument('--rec-loss-factor', type=float, default=1., help='Weighting factor for reconstruction loss.')\n",
        "    parser.add_argument('--perceptual-loss-factor', type=float, default=1., help='Weighting factor for perceptual loss.')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    args.dataset_path = r\"/content/drive/MyDrive/rbcdsai/FDU_HairFollicleDataset/Images\"\n",
        "\n",
        "train_transformer = TrainTransformer(args)"
      ],
      "metadata": {
        "id": "0Lbx7BJoqE0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pmW8hJIfrg0z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}